# -*- coding: utf-8 -*-
"""Deep Learning Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VtGy556wfW2qAMyIwNIZwChGbFHJWuPx
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers, models
from keras.applications import ResNet50
from keras.models import Model
from keras.layers import GlobalAveragePooling2D, Dense

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import mean_squared_error

print("Libraries imported")

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# loading the training and test data
try:
  train_df = pd.read_csv("/content/drive/MyDrive/Dataset/handwritten_digits/mnist_train.csv")
  test_df = pd.read_csv("/content/drive/MyDrive/Dataset/handwritten_digits/mnist_test.csv")
  print("Training and testing data loaded successfully!\n")

  # Check for NaN values and drop rows containing them
  initial_train_rows = train_df.shape[0]
  initial_test_rows = test_df.shape[0]
  train_df.dropna(inplace=True)
  test_df.dropna(inplace=True)
  print(f"Removed {initial_train_rows - train_df.shape[0]} rows with NaN values from training data.")
  print(f"Removed {initial_test_rows - test_df.shape[0]} rows with NaN values from testing data.")

  # viewing the first few rows of the training data
  print(train_df.head())
except:
  print("Failed to load data")

# separating the features and the label
x_train = train_df.drop(train_df.columns[0], axis = 1)
y_train = train_df[train_df.columns[0]]

x_test = test_df.drop(test_df.columns[0], axis=1)
y_test = test_df[test_df.columns[0]]

# check features
print(f"Training features shape: {x_train.shape}")
print(f"Labels count: {y_train.shape}")
print(f"Training features shape: {x_test.shape}")
print(f"Labels count: {y_test.shape}")

# normalizing the pixels
x_train_normalized = x_train.values / 255.0
x_test_nomalized = x_test.values / 255.0

print("Data normalized")

# check new min and max values
print(f"Original max value: {x_train.values.max()}")
print(f"Normalized max value: {x_train_normalized.max()}")

# reshaping the data to be 28x28 images since features = 784
# converting into grayscale images (-1, 28, 28, 1) (numImages, height, width, color channels)
x_train_reshape = x_train_normalized.reshape(-1, 28, 28, 1)
x_test_reshape = x_test_nomalized.reshape(-1, 28, 28, 1)

print(f"Original shape: {x_train_normalized.shape}")
print(f"New shape: {x_train_reshape.shape}")

# viewing one instance for checking
plt.imshow(x_train_reshape[0], cmap='gray')
plt.title(f"Label: {y_train[0]}")
plt.show()

from keras.utils import to_categorical

# converting the training and testing data to one-hot encoded vectors
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

print("Labels converted successfully!")

# checking the changes
print(f"\nOriginal label: {y_train[0]}")
print(f"\nCategorical label: {y_train_cat[0]}")

from sklearn.model_selection import train_test_split

# splitting 80-20 train/test
X_train, X_val, Y_train, Y_val = train_test_split(
    x_train_reshape,
    y_train_cat,
    test_size = 0.2,
    random_state = 42
)

print(f"Original training images: {x_train_reshape.shape[0]}")
print(f"New training images: {X_train.shape[0]}")
print(f"New testing images: {X_val.shape[0]}")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# defining the image data generator
train_datagen = ImageDataGenerator(
    rotation_range=10,      # randomly rotate images by up to 10 degrees
    zoom_range=0.1,         # randomly zoom in by up to 10%
    width_shift_range=0.1,  # randomly shift images horizontally by 10%
    height_shift_range=0.1  # randomly shift images vertically by 10%
)

# fitting the generator to our training data
train_datagen.fit(X_train)

print("ImageDataGenerator set up!")

# --- Model 1: Baseline (1 Dense Layer CNN [Shallow NN]) ---

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

baseline_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(10, activation='softmax')
])

print("\n--- Baseline Model Summary ---")
baseline_model.summary()

# --- Model 2: Deep Neural Network (4 Dense layers with ReLU, Dropout) ---

dnn_model = models.Sequential([
    layers.Flatten(input_shape=(28, 28,1)),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

print("\n--- DNN Model Summary ---")
dnn_model.summary()

# creating pre-trained CNN of your choice (ResNet-50)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(10, activation='softmax')(x)

resnet50_model = Model(inputs=base_model.input, outputs=predictions)
resnet50_model.summary()

# --- Model 3: Pre-trained CNN (MobileNetV2) ---

from keras.applications import MobileNetV2
from keras.layers import Input, GlobalAveragePooling2D, Lambda
from keras.models import Model

input_tensor = Input(shape=(28, 28, 1))

# Wrap tf.repeat in a Lambda layer
x = layers.Lambda(lambda image: tf.image.resize(image, (32, 32)))(input_tensor)
x_rgb = layers.Lambda(lambda image: tf.image.grayscale_to_rgb(image))(x)

mobilenet_model = MobileNetV2(
    input_tensor=x_rgb,
    weights='imagenet',
    include_top=False,
)

mobilenet_model.trainable = False

x_mobilenet = GlobalAveragePooling2D()(mobilenet_model.output)
x_mobilenet = Dense(128, activation='relu')(x_mobilenet)
output_tensor = Dense(10, activation='softmax')(x_mobilenet)

mobilenetv2_model = Model(inputs=input_tensor, outputs=output_tensor)
print("\n--- Pre-trained MobileNetV2 Model Summary ---")
mobilenetv2_model.summary()

# --- Model 3: Pre-trained CNN (ResNet50) ---
base_resnet50 = ResNet50(
    input_tensor=x_rgb,
    weights='imagenet',
    include_top=False
)
base_resnet50.trainable = False

# Add custom classifier
x_resnet50 = GlobalAveragePooling2D()(base_resnet50.output)
x_resnet50 = Dense(128, activation='relu')(x_resnet50)
output_resnet50 = Dense(10, activation='softmax')(x_resnet50)

resnet50_model = Model(inputs=input_tensor, outputs=output_resnet50)
print("\n--- Pre-trained ResNet50 Model Summary ---")
resnet50_model.summary()

from tensorflow.keras.metrics import RootMeanSquaredError

# --- 1. Compile Baseline Model ---
baseline_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', RootMeanSquaredError(name='rmse')]
)

# --- 2. Compile DNN Model ---
dnn_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', RootMeanSquaredError(name='rmse')]
)


# --- 3. Compile Resnet50 Model ---
resnet50_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', RootMeanSquaredError(name='rmse')]
)

# --- 4. Compile Mobile Net V2 Model ---
mobilenetv2_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy', RootMeanSquaredError(name='rmse')]
)

print("All models are compiled and ready to train!")

from tensorflow.keras.callbacks import EarlyStopping

print("--- Training Parameters ---")
epochs = 300
batch_size = 64
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# logs of each training run
all_history = {}



# Train and evaluate for baseline model (1 Dense Layer CNN [Shallow NN])

train_generator = train_datagen.flow(X_train, Y_train, batch_size=batch_size)

print("--- Training Baseline CNN Model ---")
history_baseline = baseline_model.fit(train_generator, epochs=300, validation_data=(X_val, Y_val), callbacks=[early_stopping], verbose=1)
print("Stopped")
print("Evaluating Baseline Model on the Test Set...")
baseline_eval = baseline_model.evaluate(x_test_reshape, y_test_cat)

all_history['baseline'] = history_baseline

# Train and evaluate for deep neural network (4 Dense layers with ReLU, Dropout)

print("--- Training Deep Neural Network Model ---")

history_dnn = dnn_model.fit(train_generator, epochs=epochs, validation_data=(X_val, Y_val), callbacks=[early_stopping], verbose=1)

print("Stopped")
print("Evaluating DNN Model on the Test Set")
dnn_eval = dnn_model.evaluate(x_test_reshape, y_test_cat)

all_history['dnn'] = history_dnn

# Train and evaluate for pre-trained CNN of your choice (MobileNetV2)

print("\n--- Training Pre-trained MobileNetV2 Model ---")
history_mobilenet = mobilenetv2_model.fit(train_generator, epochs=epochs, validation_data=(X_val, Y_val), batch_size=batch_size, callbacks=[early_stopping], verbose=1)
print("Evaluating Pre-trained Model on the Test Set...")
mobilenet_eval = mobilenetv2_model.evaluate(x_test_reshape, y_test_cat)

# Train and evaluate for pre-trained CNN of your choice (ResNet-50)

print("--- ResNet-50 Model ---")

history_resnet50 = resnet50_model.fit(train_generator, epochs=epochs, validation_data=(X_val, Y_val), callbacks=[early_stopping], verbose=1)
print("Evaluating Pre-trained Model on the Test Set")
resnet50_eval = resnet50_model.evaluate(x_test_reshape, y_test_cat)

all_history['resnet50'] = history_resnet50

# =============================================================================
# STEP 5: OBSERVATION AND ANALYSIS (Generate Outputs)
# =============================================================================
print("\nStep 5: Generating Outputs for Analysis...")

# --- 1. Final Performance Summary ---
print("\n--- Final Model Performance on Test Set ---")
print(f"Baseline Model      - Accuracy: {baseline_eval[1]:.4f}, RMSE: {baseline_eval[2]:.4f}")
print(f"DNN Model           - Accuracy: {dnn_eval[1]:.4f}, RMSE: {dnn_eval[2]:.4f}")
print(f"ResNet50 Model      - Accuracy: {resnet50_eval[1]:.4f}, RMSE: {resnet50_eval[2]:.4f}")

# --- 2. Training History Plots ---
def plot_history(history, model_name):
    plt.figure(figsize=(12, 5))
    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.suptitle(f'Training History for {model_name}', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

plot_history(history_baseline, "Baseline CNN")
plot_history(history_dnn, "Deep Neural Network")
plot_history(history_resnet50, "Pre-trained ResNet50")

# --- 3. Predicted vs. Actual Plots (Confusion Matrix) ---
from sklearn.metrics import confusion_matrix
import seaborn as sns

def plot_confusion_matrix(model, model_name):
    y_pred_probs = model.predict(x_test_reshape)
    y_pred_labels = np.argmax(y_pred_probs, axis=1)
    y_true_labels = np.argmax(y_test_cat, axis=1)
    cm = confusion_matrix(y_true_labels, y_pred_labels)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
    plt.title(f'Predicted vs. Actual - {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

print("\nGenerating Predicted vs. Actual plots (Confusion Matrices)...")
plot_confusion_matrix(baseline_model, "Baseline CNN")
plot_confusion_matrix(dnn_model, "Deep Neural Network")
plot_confusion_matrix(resnet50_model, "Pre-trained ResNet50")